import torch
import os # for deleting excess files generated by triton
from math import log2

import triton
import triton.language as tl
from triton.runtime import driver
import numpy as np

from utils import GenerateLinkedList

DEVICE = torch.device(f"cuda:{torch.cuda.current_device()}")
print("Using device:", DEVICE)

properties = driver.active.utils.get_device_properties(DEVICE.index)
NUM_SM = properties["multiprocessor_count"]
NUM_REGS = properties["max_num_regs"]
SIZE_SMEM = properties["max_shared_mem"]
WARP_SIZE = properties["warpSize"]

print(f"properties:\n{properties}")

EXAMPLE_PROGRAMS = {
  "Hello World": 0,
  "fast_eof_ll": 1
}

# kernels
@triton.jit
def add_kernel(x_ptr,  # *Pointer* to first input vector.
                y_ptr,  # *Pointer* to second input vector.
                output_ptr,  # *Pointer* to output vector.
                n_elements,  # Size of the vector.
                BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process.
                # NOTE: `constexpr` so it can be used as a shape value.
                ):
  # There are multiple 'programs' processing different data. We identify which program
  # we are here:
  pid = tl.program_id(axis=0)  # We use a 1D launch grid so axis is 0.
  # This program will process inputs that are offset from the initial data.
  # For instance, if you had a vector of length 256 and block_size of 64, the programs
  # would each access the elements [0:64, 64:128, 128:192, 192:256].
  # Note that offsets is a list of pointers:
  block_start = pid * BLOCK_SIZE
  offsets = block_start + tl.arange(0, BLOCK_SIZE)
  # Create a mask to guard memory operations against out-of-bounds accesses.
  mask = offsets < n_elements
  # Load x and y from DRAM, masking out any extra elements in case the input is not a
  # multiple of the block size.
  x = tl.load(x_ptr + offsets, mask=mask)
  y = tl.load(y_ptr + offsets, mask=mask)
  output = x + y
  # Write x + y back to DRAM.
  tl.store(output_ptr + offsets, output, mask=mask)

@triton.jit
def fast_ll_transversal(linked_list,
                        n_elements,  
                        BLOCK_SIZE: tl.constexpr,
):
  pid = tl.program_id(axis=0)
  block_start = pid * BLOCK_SIZE

  offsets = block_start + tl.arange(0, BLOCK_SIZE)
  mask = offsets < n_elements
  curr_ptr_memory = linked_list + offsets
  curr_node = tl.load(curr_ptr_memory, mask=mask, other=-1) # set to 0 if invalid

  # reuse the mask
  next_node = tl.load(linked_list + curr_node, mask=mask, other=-1)

  valid = (next_node != -1) & (curr_node != next_node) # don't store the next node if it is already stored or NULL

  tl.store(curr_ptr_memory, next_node, mask=mask)



# A wrapper around a "hello world", pulled directly from the triton documentation:
# https://triton-lang.org/main/getting-started/tutorials/01-vector-add.html#sphx-glr-getting-started-tutorials-01-vector-add-py
# This is a fairly common example 
class HelloWorld:
  def __init__(self):
    self.val = None

  def add(self, x: torch.Tensor, y: torch.Tensor):
    # We need to preallocate the output.
    output = torch.empty_like(x)
    assert x.device == DEVICE and y.device == DEVICE and output.device == DEVICE
    n_elements = output.numel()
    # The SPMD launch grid denotes the number of kernel instances that run in parallel.
    # It is analogous to CUDA launch grids. It can be either Tuple[int], or Callable(metaparameters) -> Tuple[int].
    # In this case, we use a 1D grid where the size is the number of blocks:
    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']), )
    # NOTE:
    #  - Each torch.tensor object is implicitly converted into a pointer to its first element.
    #  - `triton.jit`'ed functions can be indexed with a launch grid to obtain a callable GPU kernel.
    #  - Don't forget to pass meta-parameters as keywords arguments.
    add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=1024)
    # We return a handle to z but, since `torch.cuda.synchronize()` hasn't been called, the kernel is still
    # running asynchronously at this point.
    return output

  def runKernel(self):
    torch.manual_seed(0)
    size = 98432
    x = torch.rand(size, device=DEVICE)
    y = torch.rand(size, device=DEVICE)
    output_torch = x + y
    output_triton = self.add(x, y)
    print(output_torch)
    print(output_triton)
    print(f'The maximum difference between torch and triton is '
    f'{torch.max(torch.abs(output_torch - output_triton))}')

# the log(n) linked-list transversal from 
# https://rsim.cs.illinois.edu/arch/qual_papers/systems/3.pdf 
class FastLinkedList:
  def __init__(self):
    self.val = None

  @classmethod
  def cpu_compute(cls, LL, head):
    curr_node = head
    while LL[curr_node] != -1:
      curr_node = LL[curr_node]
    return

  @classmethod
  def gpu_compute(cls, LL, size, BLOCK_SIZE):
    grid = ((size + BLOCK_SIZE-1) // BLOCK_SIZE,) # ceil the division
    depth = int(log2(size)) + 1 

    for i in range(depth):
      fast_ll_transversal[grid](LL, size, BLOCK_SIZE)

  def runKernel(self):
    torch.manual_seed(0)
    size = 10
    size = 1024*8
    #example_linkedlist = [i for i in range(1,size)]
    #example_linkedlist[-1] = -1 # set null
    #cuda0 = torch.device('cuda:0')
    #ll_tensor = torch.tensor(example_linkedlist, dtype=torch.int32, device=DEVICE)

    ll_tensor, head = GenerateLinkedList(size=size, cudaTensor=True)
    print(f"input: {ll_tensor}")
    BLOCK_SIZE = 32
    grid = ((size + BLOCK_SIZE-1) // BLOCK_SIZE,) # ceil the division
    depth = int(log2(size)) + 1 

    print("--------------------\n")
    for i in range(depth):
      fast_ll_transversal[grid](ll_tensor, size, BLOCK_SIZE)
      print(f"iter({i}): {ll_tensor}")
    print(f"--------------------\noutput: {ll_tensor}")

# NOTE that the max here in size is 2**28, so for 4-byte integers this would be
# 2**30b = 1GB which is smaller than the average "intense" video game, 
# so should be fine but be warry 
class ExampleBenchmarks:
  @staticmethod
  @triton.testing.perf_report(
      triton.testing.Benchmark(
          x_names=['size'],
          x_vals=[2**i for i in range(12, 28)],
          x_log=True,
          line_arg='provider',
          line_vals=['torch', 'triton'],
          line_names=['Torch', 'Triton'],
          styles=[('green', '-'), ('blue', '-')],
          ylabel='GB/s',
          plot_name='vector-add-performance',
          args={},
      )
  )

  # benchmark for the Hello World program
  def benchmark_add(size, provider):
    x = torch.rand(size, device=DEVICE)
    y = torch.rand(size, device=DEVICE)
    output = torch.empty_like(x)
    n_elements = x.numel()
    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)

    if provider == 'torch':
      ms = triton.testing.do_bench(lambda: x + y)
    else:
      ms = triton.testing.do_bench(lambda: add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=1024))
    gbps = lambda ms: 3 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)
    return gbps(ms)

  @staticmethod
  @triton.testing.perf_report(
      triton.testing.Benchmark(
          x_names=['size'],
          x_vals=[2**i for i in range(5, 17)],
          x_log=True,
          line_arg='provider',
          line_vals=['custom', 'triton 64', 'triton 256', 'triton 512', 'triton 1024'],
          line_names=['Custom', 'Triton 64', 'Triton 256', 'Triton 512', 'Triton 1024'],
          styles=[
            ('black', '-'),     # Custom
            ('blue', '--'),     # Triton 64
            ('green', '-.'),    # Triton 256
            ('orange', ':'),    # Triton 512
            ('red', '-')        # Triton 1024
            ],
          ylabel='GB/s',
          plot_name='Finding the End of a Linked List Performance',
          args={},
      )
  )

  # benchmark for the fast/parallel find end of linkedlist program
  def benchmark_fast_eof_ll(size, provider):
    LL, head = GenerateLinkedList(size, provider[0:6]=='triton')

    # while knowledge of the head is not necessarily required, we require it here 
    # to keep the "chum" computations uniform 
    
    if provider == "custom":
      ms = triton.testing.do_bench(lambda: FastLinkedList.cpu_compute(LL, head))
    else: # note that this doesn't require knowledge of the "head"  
      block_size = int(provider[7:])
      ms = triton.testing.do_bench(lambda: FastLinkedList.cpu_compute(LL, head))
    gbps = lambda ms: 3 * LL.numel() * LL.element_size() * 1e-9 / (ms * 1e-3)
    return gbps(ms)

  @classmethod
  def run(cls, name, print_data=True, show_plots=True):
    if name not in benchmarks:
      print(f"Benchmark '{name}' not found.")
      return
    filename = f'images/benchmarks/{name}'
    benchmarks[name].run(print_data=print_data, show_plots=show_plots, save_path=filename)
    # remove {filename}.html
    html_path = f'{filename}/results.html'
    if os.path.exists(html_path):
      print("removing html file...")
      os.remove(html_path)

# to be used in ExampleBenchmarks.run()
benchmarks = {
  "Hello World": ExampleBenchmarks.benchmark_add,
  "fast_eof_ll": ExampleBenchmarks.benchmark_fast_eof_ll
}

# can run the func stand-alone
EXAMPLE_PROGRAMS_FUNC = [
  HelloWorld,
  FastLinkedList
]

# set -1 to run all examples, otherwise a valid integer only
def run_examples(input: str):
  runBenchmarks = True
  pProgram = EXAMPLE_PROGRAMS.get(input, -1)

  if pProgram == -1:
    if runBenchmarks:
      for key, val in EXAMPLE_PROGRAMS:
        program = EXAMPLE_PROGRAMS_FUNC[val]()
        program.runKernel()
    else:
      for key in EXAMPLE_PROGRAMS:
        ExampleBenchmarks.run(name = key)
  elif 0 <= pProgram < len(EXAMPLE_PROGRAMS_FUNC):
    if runBenchmarks:
      ExampleBenchmarks.run(name = input)
    else:
      program = EXAMPLE_PROGRAMS_FUNC[pProgram]()
      program.runKernel()

run_examples("fast_eof_ll")